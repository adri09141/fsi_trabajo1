âœ¦ComparaciÃ³n General â€“ Ensayos 1, 2, 3 y 4âœ¦
================================================

â¤EvoluciÃ³n de arquitectura, capacidad, activaciÃ³n, regularizaciÃ³n y filosofÃ­a de diseÃ±o

Los cuatro ensayos representan una lÃ­nea progresiva de experimentaciÃ³n donde se estudia cÃ³mo cambia el rendimiento y comportamiento de una CNN al modificar la profundidad, la cantidad de filtros, la estructura del clasificador, la activaciÃ³n y el optimizador.
Los Ensayos 2 y 4 son rediseÃ±os mÃ¡s profundos respecto a sus contrapartes (1 y 3), pero cada pareja explora enfoques distintos.

------------------------------------------------------------------------------------------------

ğŸ’ 1. Arquitectura general: de modelos convencionales a arquitecturas minimalistas

Los Ensayos 1 y 2 forman una pareja donde el Ensayo 2 simplifica y refina el modelo previo:
EliminaciÃ³n de clasificadores densos grandes.
Uso de 4 bloques convolucionales 16â†’32â†’32â†’64 con BatchNorm, Mish y Dropout2d.
ClasificaciÃ³n basada en GAP + Linear, mucho mÃ¡s ligera y estable.

Los Ensayos 3 y 4, en cambio, analizan la arquitectura desde una perspectiva distinta:
El Ensayo 3 usa una arquitectura corta y convencional (32â†’64).
El Ensayo 4 profundiza mucho mÃ¡s (5 capas) pero con filtros extremadamente pequeÃ±os (1â†’2â†’4â†’8â†’16), llevando el minimalismo al lÃ­mite.

ğŸ”¹Diferencia clave entre parejas:

La pareja 1â€“2 busca estabilidad, ligereza razonable y eficiencia.
La pareja 3â€“4 busca experimentar con reducciÃ³n extrema de capacidad y profundidad inusual.

------------------------------------------------------------------------------------------------

ğŸ’ 2. Capacidad convolucional: estrategias opuestas en ambos grupos

ğŸŸ¦ Ensayos 1â€“2: reducciÃ³n moderada pero estratÃ©gica
El Ensayo 2 mantiene una arquitectura â€œnormalâ€:
NÃºmero de filtros razonable (16â€“64).
Bloque doble de 32 para mejorar gradientes y detalle fino.
Capacidad suficiente para reconocer gestos complejos.

ğŸŸ§ Ensayos 3â€“4: reducciÃ³n drÃ¡stica
El Ensayo 4 reformula por completo la capacidad al pasar a:
5 capas muy pequeÃ±as: 1â†’2â†’4â†’8â†’16.
RepresentaciÃ³n extremadamente compacta (solo 16 features finales).

ğŸ”¹Dos filosofÃ­as distintas:
Ensayo 2: â€œligero pero competenteâ€.
Ensayo 4: â€œmÃ­nimo absoluto para estudiar lÃ­mitesâ€.

------------------------------------------------------------------------------------------------

ğŸ’ 3. Funciones de activaciÃ³n: comparaciÃ³n de tres enfoques

Los ensayos exploran diferentes activaciones segÃºn la arquitectura:

ğŸŸ¦ Ensayo 2
Mish, elegida por su suavidad y mejor propagaciÃ³n del gradiente en modelos pequeÃ±os/medianos.

ğŸŸ§ Ensayo 3
SiLU, una activaciÃ³n suave bien establecida para CNN de tamaÃ±o moderado.

ğŸŸ© Ensayo 4
GELU, que suele funcionar mejor en redes mÃ¡s profundas gracias a su no linealidad mÃ¡s expresiva.

ğŸ”¹El contraste global muestra que:
Ensayo 2 prioriza estabilidad y riqueza de representaciÃ³n.
Ensayo 3 mantiene una opciÃ³n estÃ¡ndar.
Ensayo 4 busca compensar la baja capacidad con una activaciÃ³n mÃ¡s fuerte.

------------------------------------------------------------------------------------------------

ğŸ’ 4. RegularizaciÃ³n y compresiÃ³n espacial

ğŸŸ¦ Ensayo 2
Implementa un enfoque equilibrado:
Dropout2d(0.1) + Dropout(0.3).
GAP a 1Ã—1 tras convoluciones de tamaÃ±o razonable.

ğŸŸ§ Ensayo 3
Utiliza GAP a 2Ã—2, conservando algo mÃ¡s de informaciÃ³n espacial.

ğŸŸ©Ensayo 4
Lleva la compresiÃ³n al extremo:
GAP a 1Ã—1 a pesar de tener muy pocos filtros.
Esto crea una representaciÃ³n ultra compacta (solo 16 valores).

La pareja 3â€“4 explora especÃ­ficamente cuÃ¡nto detalle puede eliminarse sin destruir rendimiento.
La pareja 1â€“2 busca una regularizaciÃ³n moderada y estable.

------------------------------------------------------------------------------------------------

ğŸ’ 5. Optimizadores: distintas elecciones segÃºn la arquitectura

ğŸŸ¦ Ensayo 2 utiliza AdamW, ideal para separar gradiente y regularizaciÃ³n.

ğŸŸ§ Ensayo 3 utiliza RMSprop.

ğŸŸ© Ensayo 4 cambia a Adamax, optimizado para gradientes ruidosos y modelos pequeÃ±os.

La pareja 3â€“4 explora si cambiar el optimizador puede estabilizar arquitecturas muy pequeÃ±as.
La pareja 1â€“2 se centra mÃ¡s en robustez y generalizaciÃ³n.

------------------------------------------------------------------------------------------------

ğŸ’ 6. Procesamiento de datos y tamaÃ±o de imagen

Solo los Ensayos 1â€“2 mencionan explÃ­citamente modificaciones de data augmentation y resoluciÃ³n.

Ensayo 2 reduce la resoluciÃ³n a 96Ã—96 y suaviza el augmentation.

La pareja 3â€“4 no reporta cambios en este aspecto.

ğŸ”¹En la comparativa general: los Ensayos 1â€“2 dedican mÃ¡s atenciÃ³n al preprocesamiento como parte del diseÃ±o.

â–ConclusiÃ³n generalâ–

Los 4 ensayos reflejan dos lÃ­neas de investigaciÃ³n paralelas:

ğŸŸ¦ Ensayos 1 y 2
Buscan optimizar una arquitectura razonablemente pequeÃ±a, logrando: menos parÃ¡metros, mejor estabilidad, mejor generalizaciÃ³n, un clasificador mucho mÃ¡s eficiente.
El Ensayo 2 representa una versiÃ³n pulida, ligera y equilibrada del Ensayo 1.

ğŸŸ§ Ensayos 3 y 4
Exploran el extremo del minimalismo: filtros mÃ­nimos, profundidad mÃ¡xima para la cantidad de canales, compresiÃ³n agresiva, activaciones y optimizadores alternativos.
El Ensayo 4 â€œestresaâ€ el modelo para medir los lÃ­mites de cuÃ¡nta capacidad se puede sacrificar manteniendo un comportamiento razonable.
