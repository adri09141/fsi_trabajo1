                                          ============================================
                                                ‚ú¶Comparaci√≥n General ‚Äì Ensayos‚ú¶       
                                          ============================================

‚û§Este repositorio contiene varios ensayos de redes neuronales convolucionales para clasificar el alfabeto ASL. 
‚û§Cada ensayo incluye su propio modelo, configuraci√≥n y resultados, permitiendo comparar c√≥mo cambian el rendimiento y la estabilidad al modificar arquitectura, activaci√≥n, regularizaci√≥n y optimizador.

üí†Se muestra un resumen de los ensayos ordenados de mejor a peor rendimiento

-----------------------------------------------------------------------------------------------------------------------------

üöÄEnsayo Preentrenado - EfficientNet-B0

-Transfer learning desde ImageNet.
-Solo se ajustan los √∫ltimos bloques(7 y 8).
-El clasificador final sustituido por una capa lineal de 29 clases.
-Excelente equilibrio entre rendimiento y coste computacional (~5.3M par√°metros).
-Aumentos de datos avanzados: RandomCrop, Rotation, Flip, ColorJitter.
-Optimizaci√≥n con AdamW + CrossEntropyLoss.
-Mayor precisi√≥n y capacidad de generalizaci√≥n obtenida.

üèãÔ∏è‚Äç‚ôÇÔ∏èEnsayo 1 - Arquitectura Optimizada

-CNN progresiva 16‚Üí32‚Üí64‚Üí128.
-LazyConv2D + BatchNorm + ReLU + MaxPooling.
-Dropout2D + Dropout para regularizaci√≥n.
-Clasificador denso: 1024 ‚Üí 256 ‚Üí salida (con BatchNorm y ReLU).
-Aumentos completos: Resize 128, Crop, Flip, Rotaci√≥n ¬±15¬∞, ColorJitter.
-Entrenamiento estable: Adam (wd=1e-4) + ReduceLROnPlateau.
-Modelo final equilibrado y estable sin preentrenado.

‚ö°Ô∏èEnsayo 2 - CNN Ligera y Equilibrada

-Activaci√≥n Mish para mejorar suavidad del gradiente.
-Dropout2D (0.1) en los bloques y Dropout (0.3) en el clasificador.
-AdaptiveAvgPool2d(1√ó1) para reducir par√°metros.
-Optimizaci√≥n con AdamW (lr=1e-3).
-Aumentos geom√©tricos suaves (Crop, Flip, Rotaci√≥n).
-R√°pida, eficiente y con muy buen rendimiento para su tama√±o.

üìâEnsayo 3 - CNN Profunda y Estrecha

-Activaci√≥n GELU, ideal para redes profundas con canales reducidos.
-Dropout (0.2) en el clasificador.
-AdaptiveAvgPool2d(1√ó1).
-Optimizaci√≥n con Adamax (lr=1e-3).
-Aumentos m√°s completos: Crop, Flip, Rotaci√≥n.
-Usa CosineAnnealingLR para una reducci√≥n suave del LR.
-Explora el l√≠mite inferior de capacidad con buena estabilidad; mejora respecto al Ensayo 4.

‚ò†Ô∏èEnsayo 4 ‚Äî CNN Muy Simple 

-Solo 2 bloques convolucionales: 32‚Üí64.
-Activaci√≥n SiLU, BatchNorm y MaxPool.
-Dropout moderado en el clasificador.
-AdaptiveAvgPool2d(2√ó2).
-Optimizaci√≥n con RMSprop (lr=5e-4).
-Aumentos m√≠nimos: Flip horizontal y normalizaci√≥n.
-Utiliza LinearLR para un calentamiento progresivo del learning rate.
-Modelo base usado como referencia.
