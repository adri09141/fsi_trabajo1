üß™ Comparativa principal entre Ensayo 1 y Ensayo 3

En este segundo ensayo se realizaron ajustes estructurales y de optimizaci√≥n con el objetivo de simplificar la red, mejorar la estabilidad del entrenamiento y 
reducir el n√∫mero total de par√°metros, manteniendo un buen poder de representaci√≥n para la clasificaci√≥n de letras en lenguaje de signos.

üîπ 1. Arquitectura general

- Antes (Ensayo 1)

  Red convolucional con tres bloques Conv‚ÄìBatchNorm‚ÄìReLU‚ÄìPool y un clasificador totalmente conectado con tres capas densas (fc1, fc2, fc3).

- Ahora (Ensayo 3)

  Se ampli√≥ la parte convolucional a cuatro bloques (mayor profundidad), pero se elimin√≥ el clasificador denso y se reemplaz√≥ por una 
  combinaci√≥n de Global Average Pooling (GAP) seguido de una sola capa Linear.

Justificaci√≥n:
El uso de nn.AdaptiveAvgPool2d((1, 1)) permite condensar la informaci√≥n espacial de cada canal sin necesidad de aplanar todo el tensor, reduciendo as√≠ millones de par√°metros de las capas densas.
Esto da como resultado un modelo:
- M√°s compacto
- M√°s r√°pido de entrenar
- Con menor riesgo de sobreajuste

üîπ 2. Capacidad convolucional

- Antes:

  √öltimo bloque con 64 canales tras tres convoluciones (conv1‚Äìconv3).

- Ahora:

  Se a√±adi√≥ una cuarta capa convolucional (conv4) para llegar tambi√©n a 64 canales, pero distribuyendo mejor la extracci√≥n de caracter√≠sticas (8 ‚Üí 16 ‚Üí 32 ‚Üí 64).

Beneficio:
Este escalado progresivo permite una mejor jerarqu√≠a de representaci√≥n visual y aprovecha mejor la profundidad de la red antes del pooling global.

üîπ 3. Clasificador final

- Antes (Ensayo 1):

  self.fc1 = nn.LazyLinear(1024)
  self.bn_fc1 = nn.BatchNorm1d(1024)
  self.fc2 = nn.Linear(1024, 256)
  self.bn_fc2 = nn.BatchNorm1d(256)
  self.fc3 = nn.Linear(256, num_classes)

- Ahora (Ensayo 3):

  self.gap = nn.AdaptiveAvgPool2d((1, 1))
  self.fc = nn.Linear(64, num_classes)

Justificaci√≥n:
El nuevo clasificador con GAP:
- Reduce enormemente los par√°metros entrenables.
- Aumenta la regularizaci√≥n impl√≠cita.
- Hace que la red dependa m√°s de las activaciones convolucionales que de las capas densas, mejorando la generalizaci√≥n.

üîπ 4. Optimizador

- Antes: optim.Adam(lr=0.001, weight_decay=1e-4)

- Ahora: optim.AdamW(lr=0.002, weight_decay=1e-4)

Justificaci√≥n:
AdamW mejora el control de la regularizaci√≥n al separar el weight decay del gradiente. Esto evita un mal ajuste del peso y suele ofrecer:
- Entrenamientos m√°s estables
- Mejor desempe√±o en validaci√≥n
- Convergencia m√°s predecible en redes con BatchNorm


