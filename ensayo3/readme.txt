ğŸ§ª Comparativa principal entre Ensayo 1 y Ensayo 3

En este tercer ensayo se realizaron ajustes estructurales, funcionales y de activaciÃ³n con el objetivo de ligerizar el modelo, mejorar la estabilidad del aprendizaje y mantener la capacidad de representaciÃ³n necesaria para la detecciÃ³n precisa de letras en lenguaje de signos.

ğŸ”¹ 1. Arquitectura general

- Antes (Ensayo 1):
  Red convolucional con tres bloques Convâ€“BatchNormâ€“ReLUâ€“Pool y un clasificador totalmente conectado con tres capas densas (fc1, fc2, fc3).

- Ahora (Ensayo 3):
  Se ampliÃ³ la parte convolucional a cuatro bloques (16 â†’ 32 â†’ 32 â†’ 64) para una extracciÃ³n de caracterÃ­sticas mÃ¡s jerÃ¡rquica, y se eliminÃ³ el clasificador denso en   favor de una etapa de Global Average Pooling (GAP) seguida de una sola capa lineal.

JustificaciÃ³n:
El uso de nn.AdaptiveAvgPool2d((1, 1)) permite condensar la informaciÃ³n espacial sin necesidad de aplanar todo el tensor, reduciendo millones de parÃ¡metros y mejorando la eficiencia computacional.
Esto hace que el modelo sea:
- MÃ¡s compacto y rÃ¡pido de entrenar
- Menos propenso al sobreajuste
- MÃ¡s generalizable en validaciÃ³n

ğŸ”¹ 2. Capacidad convolucional

- Antes:
  Tres capas convolucionales (16 â†’ 32 â†’ 64) seguidas de capas densas con mÃ¡s de 1 millÃ³n de parÃ¡metros.

- Ahora:
  Cuatro capas convolucionales (16 â†’ 32 â†’ 32 â†’ 64), todas normalizadas con BatchNorm2d y activadas con Mish.

Beneficio:
Este patrÃ³n progresivo permite extraer caracterÃ­sticas visuales mÃ¡s ricas sin recurrir a capas densas costosas.
La repeticiÃ³n de dos bloques con 32 canales estabiliza el flujo de gradiente y mejora la sensibilidad a variaciones sutiles en las formas de las manos.

ğŸ”¹ 3. FunciÃ³n de activaciÃ³n

- Antes (Ensayo 1): nn.ReLU()

- Ahora (Ensayo 3): nn.Mish()

JustificaciÃ³n:
Mish es una activaciÃ³n mÃ¡s suave y continua que ReLU, definida como x * tanh(softplus(x)).
Proporciona una mejor propagaciÃ³n de gradientes en valores negativos, facilitando una convergencia mÃ¡s estable y mejor precisiÃ³n final, especialmente en tareas visuales complejas como la interpretaciÃ³n de gestos o letras manuales.

ğŸ”¹ 4. RegularizaciÃ³n

- Antes: nn.Dropout(0.3)

- Ahora: nn.Dropout(0.15)

JustificaciÃ³n:
La reducciÃ³n del dropout rate es coherente con la simplificaciÃ³n del modelo.
Con menos capas densas, el riesgo de sobreajuste disminuye, por lo que un valor moderado (0.15) mantiene la regularizaciÃ³n sin afectar la retenciÃ³n de caracterÃ­sticas relevantes.

ğŸ”¹ 5. Clasificador final

- Antes (Ensayo 1):

  self.fc1 = nn.LazyLinear(1024)
  self.bn_fc1 = nn.BatchNorm1d(1024)
  self.fc2 = nn.Linear(1024, 256)
  self.bn_fc2 = nn.BatchNorm1d(256)
  self.fc3 = nn.Linear(256, num_classes)

- Ahora (Ensayo 3):

  self.gap = nn.AdaptiveAvgPool2d((1, 1))
  self.fc = nn.Linear(64, num_classes)

JustificaciÃ³n:
El nuevo clasificador reduce enormemente el nÃºmero de parÃ¡metros y prioriza la informaciÃ³n proveniente de las capas convolucionales, lo que mejora la generalizaciÃ³n y la estabilidad de la validaciÃ³n.

ğŸ”¹ 6. Optimizador

- Antes: optim.Adam(lr=0.001, weight_decay=1e-4)

- Ahora: optim.AdamW(lr=0.002, weight_decay=1e-4)

JustificaciÃ³n:
AdamW separa correctamente la penalizaciÃ³n por pesos del cÃ¡lculo del gradiente, lo que produce un entrenamiento mÃ¡s estable y mejor control de regularizaciÃ³n.
Esto es especialmente Ãºtil en redes con BatchNorm y Mish, que tienden a generar gradientes mÃ¡s suaves.
