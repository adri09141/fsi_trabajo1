ğŸ§ª Ensayo 1 â€“ Arquitectura base del modelo CNN

En este primer ensayo se diseÃ±Ã³ una arquitectura convolucional base enfocada en establecer una lÃ­nea de referencia para los siguientes experimentos.
El modelo combina capas convolucionales con normalizaciÃ³n, activaciones ReLU y un bloque denso final para la clasificaciÃ³n.

ğŸ”¹ 1. Estructura general de la red

La red se compone de tres bloques convolucionales seguidos de un bloque totalmente conectado (fully connected).
Cada bloque convolucional aplica la siguiente secuencia:

  ConvoluciÃ³n â†’ Batch Normalization â†’ ReLU â†’ MaxPooling

Esto permite:
- Extraer caracterÃ­sticas espaciales relevantes.
- Normalizar la activaciÃ³n de cada lote, acelerando el entrenamiento.
- Reducir progresivamente el tamaÃ±o espacial de las imÃ¡genes, concentrando la informaciÃ³n.
- El uso de nn.MaxPool2d(kernel_size=2, stride=2) reduce a la mitad las dimensiones despuÃ©s de cada convoluciÃ³n, facilitando un aprendizaje jerÃ¡rquico de patrones.

ğŸ”¹ 2. FunciÃ³n de activaciÃ³n

- Usada: nn.ReLU()

JustificaciÃ³n:
ReLU (Rectified Linear Unit) es una funciÃ³n de activaciÃ³n ampliamente utilizada por su simplicidad y eficiencia.
Presenta las siguientes ventajas:
- Reduce el problema del gradiente desapareciente.
- Acelera la convergencia.
- Introduce no linealidad sin aumentar demasiado el costo computacional.
Sin embargo, puede presentar el problema del â€œdying ReLUâ€, en el que ciertas neuronas dejan de activarse si sus pesos se saturan en valores negativos.

ğŸ”¹ 3. Capa de clasificaciÃ³n (Fully Connected Block)

El modelo original utiliza tres capas densas consecutivas con normalizaciÃ³n por lotes y dropout, con la estructura:
  fc1 â†’ BatchNorm â†’ ReLU â†’ Dropout  
  fc2 â†’ BatchNorm â†’ ReLU â†’ Dropout  
  fc3 â†’ ClasificaciÃ³n final

Este bloque permite al modelo:
- Combinar las caracterÃ­sticas extraÃ­das por las convoluciones.
- Aprender relaciones no lineales entre los mapas de activaciÃ³n.
- Realizar la predicciÃ³n final para las num_classes categorÃ­as.
- El uso de BatchNorm1d y Dropout(0.3) reduce el sobreajuste y estabiliza el aprendizaje, a costa de un mayor nÃºmero de parÃ¡metros.

ğŸ”¹ 4. Optimizador

- Usado: optim.Adam(lr=0.001, weight_decay=1e-4)

JustificaciÃ³n:
Adam combina las ventajas de AdaGrad y RMSProp, ajustando dinÃ¡micamente la tasa de aprendizaje por parÃ¡metro.
Es un optimizador eficiente y ampliamente utilizado en redes profundas debido a su rÃ¡pida convergencia y estabilidad.
El parÃ¡metro weight_decay introduce una ligera regularizaciÃ³n L2 para prevenir sobreajuste.

ğŸ”¹ 5. RegularizaciÃ³n

- Usado: nn.Dropout2d(0.1)

JustificaciÃ³n:
El uso del dropout2d + dropout ya que: 
- Dropout2d apaga canales completos (feature maps) en una capa convolucional.
- Mientras que Dropout â€œnormalâ€ apaga neuronas individuales aleatoriamente.

Esto nos permite reducir el sobreajuste y mejorar la generalizaciÃ³n del modelo.
