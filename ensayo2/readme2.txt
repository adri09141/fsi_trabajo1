ğŸ§ª Ensayo 2 â€“ CNN ligera 4 bloques (16â€“32â€“32â€“64) con AdamW

En este segundo ensayo se realizÃ³ una reestructuraciÃ³n profunda de la arquitectura con el objetivo de ligerizar el modelo, mejorar la estabilidad del aprendizaje y conservar la capacidad de representaciÃ³n necesaria para reconocer letras del lenguaje de signos de forma robusta.

ğŸ”¹ 1. Arquitectura general

Se diseÃ±Ã³ una red convolucional con 4 bloques de procesamiento:

16 â†’ 32 â†’ 32 â†’ 64

Cada bloque incluye:

- Conv2d
- BatchNorm2d
- ActivaciÃ³n Mish
- MaxPool2d
- Dropout2d moderado 

DespuÃ©s del cuerpo convolucional se aplica:
- Global Average Pooling (GAP)
- Dropout
- Una sola capa lineal (64 â†’ num_classes)

Esto elimina por completo los clasificadores densos grandes y hace que toda la capacidad provenga de las convoluciones.

Beneficios:
- Mucho menos parÃ¡metros totales
- Mejor generalizaciÃ³n
- Entrenamiento mÃ¡s estable
- Menor riesgo de sobreajuste

ğŸ”¹ 2. Capacidad convolucional

El uso de 4 bloques con un patrÃ³n progresivo
  16 â†’ 32 â†’ 32 â†’ 64
permite extraer caracterÃ­sticas visuales mÃ¡s profundas sin volver el modelo pesado.

El bloque doble de 32 canales mejora:

- estabilidad del gradiente
- sensibilidad a detalles finos en la mano
- precisiÃ³n en gestos complejos

ğŸ”¹ 3. FunciÃ³n de activaciÃ³n: Mish

Se reemplazÃ³ ReLU por Mish, una activaciÃ³n suave y continua que:

- conserva informaciÃ³n en valores negativos
- mejora la propagaciÃ³n del gradiente
- ayuda a modelos pequeÃ±os/medianos a converger mejor
- produce representaciones mÃ¡s ricas para visiÃ³n

ğŸ”¹ 4. RegularizaciÃ³n

El Ensayo 2 combina dos formas de regularizaciÃ³n:
- Dropout2d(0.1) en convoluciones
- Dropout(0.3) en la capa final

Esto estabiliza el entrenamiento sin inhibir la capacidad de representaciÃ³n.

ğŸ”¹ 5. Clasificador final (GAP + Linear)

En lugar de mÃºltiples capas densas, ahora se utiliza:
  AdaptiveAvgPool2d((1,1))
  Flatten
  Linear(64 â†’ num_classes)

Ventajas:
- reducciÃ³n drÃ¡stica de parÃ¡metros
- mejor uso de la informaciÃ³n convolucional
- red mÃ¡s rÃ¡pida y mÃ¡s robusta

ğŸ”¹ 6. Optimizador: AdamW

Se adoptÃ³ AdamW con:

  lr = 0.001
  weight_decay = 1e-4

Beneficios:
- separa la regularizaciÃ³n del gradiente
- mejora la estabilidad del entrenamiento
- alcanza mejor generalizaciÃ³n

ğŸ”¹ 7. Transformaciones de entrenamiento

Se ajustÃ³ el esquema de data augmentation para que sea suave pero efectivo:
  
  Resize(96Ã—96)
  RandomCrop(padding=4)
  RandomHorizontalFlip(0.5)
  RandomRotation(5Â°)
  ToTensor()
  Normalize(...)


Cambios clave:
- menor rotaciÃ³n (5Â°) para no deformar el gesto
- menor padding (4) para mantener la mano centrada
- se elimina ColorJitter para evitar ruido innecesario

ğŸ”¹ 8. TamaÃ±o de imagen: 96Ã—96

La resoluciÃ³n se redujo a 96Ã—96, ofreciendo:
- entrenamiento mÃ¡s rÃ¡pido
- menor memoria
- suficiente detalle para distinguir gestos
- menor tendencia al sobreajuste
