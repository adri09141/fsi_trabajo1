ğŸ§ª Comparativa principal entre Ensayo 1 y Ensayo 

En este segundo ensayo se realizaron ajustes estructurales, funcionales y de activaciÃ³n con el objetivo de ligerizar el modelo, mejorar la estabilidad del aprendizaje y mantener la capacidad de representaciÃ³n necesaria para la detecciÃ³n precisa de letras en lenguaje de signos.

ğŸ”¹ 1. Arquitectura general

- Antes (Ensayo 1):
  Red convolucional con tres bloques Convâ€“BatchNormâ€“ReLUâ€“Pool y un clasificador totalmente conectado con tres capas densas (fc1, fc2, fc3).

- Ahora (Ensayo 2):
  Se ampliÃ³ la parte convolucional a cuatro bloques (16 â†’ 32 â†’ 32 â†’ 64) para una extracciÃ³n de caracterÃ­sticas mÃ¡s jerÃ¡rquica, y se eliminÃ³ el clasificador denso en favor de una etapa de Global Average Pooling (GAP) seguida de una sola capa lineal.

JustificaciÃ³n:
El uso de nn.AdaptiveAvgPool2d((1, 1)) permite condensar la informaciÃ³n espacial sin necesidad de aplanar todo el tensor, reduciendo millones de parÃ¡metros y mejorando la eficiencia computacional.
Esto hace que el modelo sea:
- MÃ¡s compacto y rÃ¡pido de entrenar
- Menos propenso al sobreajuste
- MÃ¡s generalizable en validaciÃ³n

ğŸ”¹ 2. Capacidad convolucional

- Antes:
  Tres capas convolucionales (16 â†’ 32 â†’ 64) seguidas de capas densas con mÃ¡s de 1 millÃ³n de parÃ¡metros.

- Ahora:
  Cuatro capas convolucionales (16 â†’ 32 â†’ 32 â†’ 64), todas normalizadas con BatchNorm2d y activadas con Mish.

Beneficio:
Este patrÃ³n progresivo permite extraer caracterÃ­sticas visuales mÃ¡s ricas sin recurrir a capas densas costosas.
La repeticiÃ³n de dos bloques con 32 canales estabiliza el flujo de gradiente y mejora la sensibilidad a variaciones sutiles en las formas de las manos.

ğŸ”¹ 3. FunciÃ³n de activaciÃ³n

- Antes (Ensayo 1): nn.ReLU()

- Ahora (Ensayo 2): nn.Mish()

JustificaciÃ³n:
Mish es una activaciÃ³n mÃ¡s suave y continua que ReLU, definida como x * tanh(softplus(x)).
Proporciona una mejor propagaciÃ³n de gradientes en valores negativos, facilitando una convergencia mÃ¡s estable y mejor precisiÃ³n final, especialmente en tareas visuales complejas como la interpretaciÃ³n de gestos o letras manuales.

ğŸ”¹ 4. RegularizaciÃ³n

- Antes: nn.Dropout(0.3)

- Ahora: nn.Dropout(0.15)

JustificaciÃ³n:
La reducciÃ³n del dropout rate es coherente con la simplificaciÃ³n del modelo.
Con menos capas densas, el riesgo de sobreajuste disminuye, por lo que un valor moderado (0.15) mantiene la regularizaciÃ³n sin afectar la retenciÃ³n de caracterÃ­sticas relevantes.

ğŸ”¹ 5. Clasificador final

- Antes (Ensayo 1):

  self.fc1 = nn.LazyLinear(1024)
  self.bn_fc1 = nn.BatchNorm1d(1024)
  self.fc2 = nn.Linear(1024, 256)
  self.bn_fc2 = nn.BatchNorm1d(256)
  self.fc3 = nn.Linear(256, num_classes)

- Ahora (Ensayo 2):

  self.gap = nn.AdaptiveAvgPool2d((1, 1))
  self.fc = nn.Linear(64, num_classes)

JustificaciÃ³n:
El nuevo clasificador reduce enormemente el nÃºmero de parÃ¡metros y prioriza la informaciÃ³n proveniente de las capas convolucionales, lo que mejora la generalizaciÃ³n y la estabilidad de la validaciÃ³n.

ğŸ”¹ 6. Optimizador

- Antes: optim.Adam(lr=0.001, weight_decay=1e-4)

- Ahora: optim.AdamW(lr=0.002, weight_decay=1e-4)

JustificaciÃ³n:

AdamW separa correctamente la penalizaciÃ³n por pesos del cÃ¡lculo del gradiente, lo que produce un entrenamiento mÃ¡s estable y mejor control de regularizaciÃ³n.
Esto es especialmente Ãºtil en redes con BatchNorm y Mish, que tienden a generar gradientes mÃ¡s suaves.

ğŸ”¹ 7. Train_transform

- Antes (Ensayo 1):

  train_transform = transforms.Compose([
      transforms.Resize(img_size),
      transforms.RandomCrop(img_size, padding=8),  # mÃ¡s barato que RandomResizedCrop
      transforms.RandomHorizontalFlip(p=0.5),
      transforms.RandomRotation(10),               # reemplaza RandomAffine
      transforms.ColorJitter(
          brightness=0.1,
          contrast=0.1,
          saturation=0.05
      ),
      transforms.ToTensor(),
      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
  ])

- Ahora (Ensayo 2):

  train_transform = transforms.Compose([
      transforms.Resize(img_size),
      transforms.RandomCrop(img_size, padding=4),
      transforms.RandomHorizontalFlip(p=0.5),
      transforms.RandomRotation(5),
      transforms.ToTensor(),
      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
  ])

JustificaciÃ³n:

El nuevo esquema de data augmentation busca un equilibrio entre diversidad y consistencia visual, reduciendo el grado de aleatoriedad aplicado a las imÃ¡genes para evitar que el modelo aprenda patrones deformados o irreales de las manos.
Los cambios principales y su impacto:
  ğŸ”¸ ReducciÃ³n de padding en el recorte (8 â†’ 4):
  Menor desplazamiento aleatorio del contenido visual. Favorece que las manos se mantengan centradas, lo que ayuda a conservar las proporciones naturales del gesto.

  ğŸ”¸ RotaciÃ³n mÃ¡s leve (10Â° â†’ 5Â°):
  Mejora la estabilidad de la convergencia. En lenguaje de signos, las rotaciones excesivas pueden alterar completamente el significado del gesto.

  ğŸ”¸ EliminaciÃ³n de ColorJitter:
  Aunque Ãºtil para iluminaciÃ³n variable, su eliminaciÃ³n evita introducir ruido cromÃ¡tico innecesario, ya que las condiciones de captura en el dataset son       relativamente homogÃ©neas.
  
  ğŸ”¸ NormalizaciÃ³n constante:
  Mantener los valores centrados en torno a cero mejora la estabilidad de las activaciones, especialmente con BatchNorm y Mish.

En conjunto, el nuevo train_transform genera un aprendizaje mÃ¡s robusto y consistente, reduciendo la variabilidad espuria mientras conserva la capacidad de generalizaciÃ³n.

ğŸ”¹ 8. img_size 

- Antes (Ensayo 1): (128, 128)

- Ahora (Ensayo 2): (96, 96)

JustificaciÃ³n:
Reducir la resoluciÃ³n a 96Ã—96 pÃ­xeles optimiza el equilibrio entre detalle visual y coste computacional.
Las letras del lenguaje de signos presentan formas bien definidas que pueden capturarse adecuadamente a esta resoluciÃ³n sin pÃ©rdida significativa de informaciÃ³n discriminante.

Ventajas:
- Entrenamiento mÃ¡s rÃ¡pido y eficiente, permitiendo mayor nÃºmero de Ã©pocas sin sobrecargar GPU.
- Menor riesgo de sobreajuste, al reducir el volumen de pÃ­xeles redundantes.
- Mejor compatibilidad con redes mÃ¡s ligeras, manteniendo una representaciÃ³n suficiente para distinguir gestos similares (como â€œMâ€ vs â€œNâ€).

