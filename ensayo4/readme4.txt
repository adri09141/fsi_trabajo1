ğŸ§ª Ensayo 4 â€“ QuÃ© cambia respecto al Ensayo 3 y por quÃ© es importante

El Ensayo 4 representa una variaciÃ³n mÃ¡s profunda y minimalista del modelo usado en el Ensayo 3, y su propÃ³sito principal es estudiar cÃ³mo cambia el comportamiento de la red cuando:

- Se aumentan las capas convolucionales.
- Se reduce drÃ¡sticamente el nÃºmero de filtros.
- Se utiliza una activaciÃ³n distinta (GELU en lugar de SiLU).
- Se comprime mÃ¡s la representaciÃ³n (GAP a 1Ã—1).
- Se cambia el optimizador (Adamax en lugar de RMSprop**).

A continuaciÃ³n se explica quÃ© aporta cada uno de estos cambios cuando comparamos directamente Ensayo 4 vs Ensayo 3.

ğŸ” 1. Profundidad: 5 capas vs 2 capas

- Ensayo 3: 2 capas (32 y 64 filtros).
- Ensayo 4: 5 capas (1â†’2â†’4â†’8â†’16 filtros).

El Ensayo 4 explora si mÃ¡s profundidad, incluso con filtros muy pequeÃ±os, puede capturar mejor patrones jerÃ¡rquicos.

â¡ï¸ HipÃ³tesis a probar:
â€œUna red mÃ¡s profunda aunque con menos filtros puede aprender mejor que una red corta con filtros mÃ¡s anchos.â€

ğŸ” 2. NÃºmero de filtros: crecimiento mÃ­nimo vs convencional

- Ensayo 3: 32 â†’ 64 (convencional).
- Ensayo 4: 1 â†’ 2 â†’ 4 â†’ 8 â†’ 16 (minimalista extremo).

AquÃ­ el Ensayo 4 lleva al lÃ­mite la idea de â€œmenos es mÃ¡sâ€:

âœ” Menos parÃ¡metros.
âœ” Menos memoria.
âœ” Menos riesgo de sobreajuste.

Pero a costa de una capacidad representacional mucho menor.

â¡ï¸ Lo que compara Ensayo 4:
Â¿Una red muy ligera puede competir en rendimiento con la arquitectura base?

ğŸ” 3. ActivaciÃ³n: GELU vs SiLU

- Ensayo 3 usa SiLU (suave, derivada estable).
- Ensayo 4 usa GELU (mÃ¡s expresiva en redes profundas).

GELU tiende a funcionar mejor cuando hay muchas capas, porque:

âœ” permite flujos de gradiente mÃ¡s adaptativos,
âœ” introduce una no linealidad mÃ¡s rica que SiLU.

â¡ï¸ El Ensayo 4 prueba:
Si la activaciÃ³n GELU compensa la baja cantidad de filtros gracias a su mayor capacidad expresiva.

ğŸ” 4. Pooling y compresiÃ³n espacial

- Ensayo 3: GAP a (2Ã—2).
- Ensayo 4: GAP a (1Ã—1).

El Ensayo 4 comprime la imagen hasta el punto mÃ¡ximo, convirtiendo todo el mapa en un Ãºnico valor por canal.

âœ” RepresentaciÃ³n sÃºper compacta.
âœ” Muy pocas caracterÃ­sticas entran al clasificador.

Pero esto implica:

âš  Se pierde informaciÃ³n espacial fina.
âš  El clasificador recibe un vector mÃ¡s pobre (solo 16 valores).

â¡ï¸ Ensayo 4 evalÃºa:
Â¿QuÃ© tan lejos se puede llevar la compresiÃ³n sin destruir el rendimiento?

ğŸ” 5. Optimizador: Adamax vs RMSprop

- Ensayo 3 usa RMSprop.
- Ensayo 4 usa Adamax.

Adamax funciona especialmente bien con:
âœ” modelos pequeÃ±os,
âœ” gradientes ruidosos,
âœ” LazyModules (como en esta arquitectura).

â¡ï¸ La pregunta del Ensayo 4 es:
Â¿Puede Adamax estabilizar la convergencia de una red muy pequeÃ±a y profunda donde RMSprop quizÃ¡ no sea Ã³ptimo?

ğŸ¯ ConclusiÃ³n centrada en Ensayo 4

El Ensayo 4 no pretende ser una red mejor que la del Ensayo 3.
Su rol es experimental: estresar el concepto de CNN minimalista para medir los lÃ­mites de:
- capacidad con muy pocos filtros,
- profundidad extrema en redes ligeras,
- compresiÃ³n agresiva de caracterÃ­sticas,
- diferentes activaciones y optimizadores en un entorno reducido.
